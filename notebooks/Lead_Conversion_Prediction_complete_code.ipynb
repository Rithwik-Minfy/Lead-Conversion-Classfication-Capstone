{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9684de52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully from PostgreSQL with shape: (9240, 37)\n",
      "                            Prospect ID  Lead Number              Lead Origin  \\\n",
      "0  7927b2df-8bba-4d29-b9a2-b6e0beafe620       660737                      API   \n",
      "1  2a272436-5132-4136-86fa-dcc88c88f482       660728                      API   \n",
      "2  8cc8c611-a219-4f35-ad23-fdfd2656bd8a       660727  Landing Page Submission   \n",
      "3  0cc2df48-7cf4-4e39-9de9-19797f9b38cc       660719  Landing Page Submission   \n",
      "4  3256f628-e534-4826-9d63-4a8b88782852       660681  Landing Page Submission   \n",
      "\n",
      "      Lead Source Do Not Email Do Not Call  Converted  TotalVisits  \\\n",
      "0      Olark Chat           No          No          0          0.0   \n",
      "1  Organic Search           No          No          0          5.0   \n",
      "2  Direct Traffic           No          No          1          2.0   \n",
      "3  Direct Traffic           No          No          0          1.0   \n",
      "4          Google           No          No          1          2.0   \n",
      "\n",
      "   Total Time Spent on Website  Page Views Per Visit  ...  \\\n",
      "0                            0                   0.0  ...   \n",
      "1                          674                   2.5  ...   \n",
      "2                         1532                   2.0  ...   \n",
      "3                          305                   1.0  ...   \n",
      "4                         1428                   1.0  ...   \n",
      "\n",
      "  Get updates on DM Content    Lead Profile    City  \\\n",
      "0                        No          Select  Select   \n",
      "1                        No          Select  Select   \n",
      "2                        No  Potential Lead  Mumbai   \n",
      "3                        No          Select  Mumbai   \n",
      "4                        No          Select  Mumbai   \n",
      "\n",
      "  Asymmetrique Activity Index Asymmetrique Profile Index  \\\n",
      "0                   02.Medium                  02.Medium   \n",
      "1                   02.Medium                  02.Medium   \n",
      "2                   02.Medium                    01.High   \n",
      "3                   02.Medium                    01.High   \n",
      "4                   02.Medium                    01.High   \n",
      "\n",
      "  Asymmetrique Activity Score Asymmetrique Profile Score  \\\n",
      "0                        15.0                       15.0   \n",
      "1                        15.0                       15.0   \n",
      "2                        14.0                       20.0   \n",
      "3                        13.0                       17.0   \n",
      "4                        15.0                       18.0   \n",
      "\n",
      "  I agree to pay the amount through cheque  \\\n",
      "0                                       No   \n",
      "1                                       No   \n",
      "2                                       No   \n",
      "3                                       No   \n",
      "4                                       No   \n",
      "\n",
      "  A free copy of Mastering The Interview Last Notable Activity  \n",
      "0                                     No              Modified  \n",
      "1                                     No          Email Opened  \n",
      "2                                    Yes          Email Opened  \n",
      "3                                     No              Modified  \n",
      "4                                     No              Modified  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from ../.env\n",
    "load_dotenv(dotenv_path=Path(\"..\") / \".env\")\n",
    "\n",
    "def load_data_from_postgres(\n",
    "    table_name,\n",
    "    db_user=os.getenv(\"DB_USER\"),\n",
    "    db_password=os.getenv(\"DB_PASSWORD\"),\n",
    "    db_host=os.getenv(\"DB_HOST\"),\n",
    "    db_port=os.getenv(\"DB_PORT\"),\n",
    "    db_name=os.getenv(\"DB_NAME\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the dataset from a PostgreSQL table and validate its structure.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table in PostgreSQL.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded dataset.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns are missing or the dataset is empty.\n",
    "    \"\"\"\n",
    "    # Encode password to be URL-safe\n",
    "    encoded_password = urllib.parse.quote_plus(db_password)\n",
    "\n",
    "    # Create connection string\n",
    "    connection_str = f'postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}'\n",
    "    engine = create_engine(connection_str)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_sql_table(table_name, con=engine)\n",
    "\n",
    "    # Check if dataset is empty\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Dataset is empty\")\n",
    "\n",
    "    # Required columns for Lead Scoring project\n",
    "    required_columns = [\n",
    "        'Prospect ID', 'Lead Number', 'Lead Origin', 'Lead Source', 'Do Not Email',\n",
    "        'Do Not Call', 'Converted', 'TotalVisits', 'Total Time Spent on Website',\n",
    "        'Page Views Per Visit', 'Last Activity', 'Country', 'Specialization',\n",
    "        'How did you hear about X Education', 'What is your current occupation',\n",
    "        'What matters most to you in choosing a course', 'Search', 'Magazine',\n",
    "        'Newspaper Article', 'X Education Forums', 'Newspaper',\n",
    "        'Digital Advertisement', 'Through Recommendations',\n",
    "        'Receive More Updates About Our Courses', 'Tags', 'Lead Quality',\n",
    "        'Update me on Supply Chain Content', 'Get updates on DM Content',\n",
    "        'Lead Profile', 'City', 'Asymmetrique Activity Index',\n",
    "        'Asymmetrique Profile Index', 'Asymmetrique Activity Score',\n",
    "        'Asymmetrique Profile Score', 'I agree to pay the amount through cheque',\n",
    "        'A free copy of Mastering The Interview', 'Last Notable Activity'\n",
    "    ]\n",
    "\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    print(\"✅ Dataset loaded successfully from PostgreSQL with shape:\", df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = load_data_from_postgres(table_name='lead_scoring_data')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e300f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Records: 1281\n",
      "Data cleaned and ready. Remaining nulls:\n",
      "Last Activity    103\n",
      "dtype: int64\n",
      "✅ Data cleaning completed\n",
      "Shape of the data after cleaning  (7959, 25)\n"
     ]
    }
   ],
   "source": [
    "# Applying the techniques observed in EDA\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    # Prospect ID and Lead Number adds no additional information so drop these columns\n",
    "    df = df.drop(columns=['Prospect ID', 'Lead Number'], errors='ignore')\n",
    "\n",
    "    # Removing as these contains constant value \"False\"\n",
    "    df = df.drop(columns=[\"Magazine\", \"Receive More Updates About Our Courses\", \"Update me on Supply Chain Content\", \"Get updates on DM Content\",\"I agree to pay the amount through cheque\"], errors='ignore')\n",
    "    \n",
    "    # Check for duplicate records\n",
    "    print(\"\\nDuplicate Records:\", df.duplicated().sum())\n",
    "\n",
    "    # if duplicate rows found then drop\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Filling the missing null values and label encoding \"Asymmetrique Profile Index\" and \"Asymmetrique Activity Index\"\n",
    "    # and dropping unneccesary columns\n",
    "    drop_cols = [\n",
    "        'Newspaper', 'Newspaper Article',\n",
    "        'X Education Forums', 'Search', 'Through Recommendations'\n",
    "    ]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    # Standardize labels\n",
    "    df['Lead Source'] = df['Lead Source'].str.strip().str.lower()\n",
    "    df['Lead Source'] = df['Lead Source'].replace({'google': 'Google'})\n",
    "    df['Lead Source'] = df['Lead Source'].fillna('Missing')\n",
    "\n",
    "    # Replace 'Select' with 'Missing'\n",
    "    select_to_missing_cols = [\n",
    "        'Specialization', 'Lead Profile', 'City', 'How did you hear about X Education'\n",
    "    ]\n",
    "    for col in select_to_missing_cols:\n",
    "        df[col] = df[col].replace('Select', 'Missing')\n",
    "        df[col] = df[col].fillna('Missing')\n",
    "\n",
    "    # Fill remaining high-null categorical columns\n",
    "    df['Country'] = df['Country'].fillna('India')\n",
    "    df['What is your current occupation'] = df['What is your current occupation'].fillna('Unemployed')\n",
    "    df['What matters most to you in choosing a course'] = df['What matters most to you in choosing a course'].fillna('Better Career Prospects')\n",
    "    df['Tags'] = df['Tags'].fillna('Missing')\n",
    "    df['Lead Quality'] = df['Lead Quality'].fillna('Missing')\n",
    "\n",
    "    # Ordinal encoding for Asymmetrique Index columns\n",
    "    ordinal_map = {\"01.High\": 3, \"02.Medium\": 2, \"03.Low\": 1}\n",
    "    df[\"Asymmetrique Profile Index\"] = df[\"Asymmetrique Profile Index\"].map(ordinal_map)\n",
    "    df[\"Asymmetrique Activity Index\"] = df[\"Asymmetrique Activity Index\"].map(ordinal_map)\n",
    "\n",
    "    # Fill with median\n",
    "    for col in [\n",
    "        'Asymmetrique Profile Index', 'Asymmetrique Activity Index',\n",
    "        'Asymmetrique Profile Score', 'Asymmetrique Activity Score',\n",
    "        'TotalVisits', 'Page Views Per Visit'\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Done\n",
    "    print(\"Data cleaned and ready. Remaining nulls:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "    # Mapping the labels of the columns with similar meaning and if more labels to convert to less labels\n",
    "    lead_source_map = {\n",
    "        'google': 'Google',\n",
    "        'google ads': 'Google',\n",
    "        'organic search': 'Organic Search',\n",
    "        'olark chat': 'Olark Chat',\n",
    "        'direct traffic': 'Direct Traffic',\n",
    "        'reference': 'Reference',\n",
    "        'welingak website': 'Welingak',\n",
    "        'facebook': 'Social Media',\n",
    "        'bing': 'Other',\n",
    "        'click2call': 'Other',\n",
    "        'press_release': 'Other',\n",
    "        'social media': 'Social Media',\n",
    "        'live chat': 'Olark Chat',\n",
    "        'youtubechannel': 'Other',\n",
    "        'testone': 'Other',\n",
    "        'pay per click ads': 'Other',\n",
    "        'welearnblog_home': 'Other',\n",
    "        'welearn': 'Other',\n",
    "        'blog': 'Other',\n",
    "        'nc_edm': 'Other'\n",
    "    }\n",
    "\n",
    "    specialization_map = {\n",
    "        'finance management': 'Finance',\n",
    "        'banking, investment and insurance': 'Finance',\n",
    "        'human resource management': 'HR',\n",
    "        'marketing management': 'Marketing',\n",
    "        'operations management': 'Operations',\n",
    "        'it projects management': 'IT',\n",
    "        'business administration': 'Business',\n",
    "        'supply chain management': 'Operations',\n",
    "        'e-commerce': 'Business',\n",
    "        'retail management': 'Marketing',\n",
    "        'media and advertising': 'Marketing',\n",
    "        'travel and tourism': 'Other',\n",
    "        'international business': 'Business',\n",
    "        'healthcare management': 'Other',\n",
    "        'hospitality management': 'Other',\n",
    "        'rural and agribusiness': 'Other',\n",
    "        'e-business': 'Business',\n",
    "        'services excellence': 'Other',\n",
    "        'missing': 'Missing',\n",
    "        'select': 'Missing'\n",
    "    }\n",
    "\n",
    "    tags_map = {\n",
    "        'will revert after reading the email': 'Reverting',\n",
    "        'interested in other courses': 'Interested Other',\n",
    "        'interested  in full time mba': 'Interested Other',\n",
    "        'graduation in progress': 'Interested Other',\n",
    "        'not doing further education': 'Not Interested',\n",
    "        'wrong number given': 'Not Reachable',\n",
    "        'opp hangup': 'Not Reachable',\n",
    "        'number not provided': 'Not Reachable',\n",
    "        'invalid number': 'Not Reachable',\n",
    "        'still thinking': 'Still Thinking',\n",
    "        'shall take in the next coming month': 'Still Thinking',\n",
    "        'want to take admission but has financial problems': 'Still Thinking',\n",
    "        'lost to eins': 'Lost',\n",
    "        'lost to others': 'Lost',\n",
    "        'in touch with eins': 'Lost',\n",
    "        'diploma holder (not eligible)': 'Not Eligible',\n",
    "        'university not recognized': 'Not Eligible',\n",
    "        'recognition issue (dec approval)': 'Not Eligible',\n",
    "        'already a student': 'Already Student',\n",
    "        'switched off': 'Not Reachable',\n",
    "        'busy': 'Not Reachable',\n",
    "        'ringing': 'Not Reachable',\n",
    "        'missing': 'Missing',\n",
    "        '': 'Missing',\n",
    "    }\n",
    "\n",
    "    lead_quality_map = {\n",
    "        'high in relevance': 'High',\n",
    "        'might be': 'Medium',\n",
    "        'not sure': 'Medium',\n",
    "        'low in relevance': 'Low',\n",
    "        'worst': 'Low',\n",
    "        'missing': 'Missing'\n",
    "    }\n",
    "\n",
    "    lead_profile_map = {\n",
    "        'potential lead': 'Potential',\n",
    "        'other leads': 'Other',\n",
    "        'student of someschool': 'Student',\n",
    "        'lateral student': 'Student',\n",
    "        'dual specialization student': 'Student',\n",
    "        'select': 'Missing',\n",
    "        'missing': 'Missing'\n",
    "    }\n",
    "\n",
    "    heard_map = {\n",
    "        'online search': 'Online',\n",
    "        'word of mouth': 'Referral',\n",
    "        'student of someschool': 'Referral',\n",
    "        'multiple sources': 'Multiple',\n",
    "        'advertisements': 'Ads',\n",
    "        'social media': 'Social',\n",
    "        'email': 'Direct',\n",
    "        'sms': 'Direct',\n",
    "        'other': 'Other',\n",
    "        'select': 'Missing',\n",
    "        'missing': 'Missing'\n",
    "    }\n",
    "\n",
    "    # Clean cell values only (not column names)\n",
    "    df = df.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Apply mappings with correct column names\n",
    "    df['Lead Source'] = df['Lead Source'].replace(lead_source_map)\n",
    "    df['Specialization'] = df['Specialization'].replace(specialization_map)\n",
    "    df['Tags'] = df['Tags'].replace(tags_map)\n",
    "    df['Lead Quality'] = df['Lead Quality'].replace(lead_quality_map)\n",
    "    df['Lead Profile'] = df['Lead Profile'].replace(lead_profile_map)\n",
    "    df['How did you hear about X Education'] = df['How did you hear about X Education'].replace(heard_map)\n",
    "    print(\"✅ Data cleaning completed\")\n",
    "    print(\"Shape of the data after cleaning \",df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df_cleaned = clean_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f554f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature engineering complete. Shape after:  (7959, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------- Feature 1: Engagement Score -----------\n",
    "    # Proxy for user engagement based on time and activity on site\n",
    "    if set(['Total Time Spent on Website', 'Page Views Per Visit', 'TotalVisits']).issubset(df.columns):\n",
    "        df['Engagement Score'] = (\n",
    "            df['Total Time Spent on Website'] * 0.4 +\n",
    "            df['Page Views Per Visit'] * 0.3 +\n",
    "            df['TotalVisits'] * 0.3\n",
    "        )\n",
    "\n",
    "    # ----------- Feature 2: Combined Asymmetrique Score -----------\n",
    "    if set(['Asymmetrique Activity Score', 'Asymmetrique Profile Score']).issubset(df.columns):\n",
    "        df['Combined Asymmetrique Score'] = (\n",
    "            df['Asymmetrique Activity Score'] + df['Asymmetrique Profile Score']\n",
    "        )\n",
    "        # Removing columns after taking the combined value as new feature\n",
    "        df.drop(columns=['Asymmetrique Activity Score', 'Asymmetrique Profile Score'])\n",
    "\n",
    "    # ----------- Feature 3: Is New Tag -----------\n",
    "    if 'Tags' in df.columns:\n",
    "        df['Is New Tag'] = df['Tags'].apply(lambda x: 1 if 'student' in str(x).lower() else 0)\n",
    "\n",
    "    # ----------- Feature 4: Interaction Level based on Activity -----------\n",
    "    if 'Last Activity' in df.columns:\n",
    "        high_activity = ['SMS Sent', 'Email Opened', 'Email Link Clicked']\n",
    "        df['High Interaction'] = df['Last Activity'].apply(lambda x: 1 if x in high_activity else 0)\n",
    "\n",
    "    # ----------- Feature 5: Was Previously Interested -----------\n",
    "    if 'Lead Profile' in df.columns:\n",
    "        df['Potential Lead'] = df['Lead Profile'].apply(lambda x: 1 if 'potential' in str(x).lower() else 0)\n",
    "\n",
    "    print(\"✅ Feature engineering complete. Shape after: \", df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df_featured = feature_engineering(df_cleaned)\n",
    "    df.to_csv(\"featured_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b032638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete. Artifacts saved to: pkl_joblib_files\n",
      "🧾 X_train shape: (6367, 145)\n",
      "🧾 X_test shape : (1592, 145)\n",
      "🎯 y_train dist:\n",
      " Converted\n",
      "0    0.600126\n",
      "1    0.399874\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Function to apply Winsorization\n",
    "def winsorize_columns(df, columns, limits=(0.01, 0.01)):\n",
    "    for col in columns:\n",
    "        try:\n",
    "            df[col] = winsorize(df[col], limits=limits)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Could not winsorize column '{col}': {e}\")\n",
    "    return df\n",
    "\n",
    "# Main preprocessing pipeline function\n",
    "def preprocess_data(df, save_dir=\"pkl_joblib_files\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------\n",
    "    # 1. Define features\n",
    "    # ----------------------\n",
    "    target_col = 'Converted'\n",
    "    \n",
    "    numeric_cols = [\n",
    "        'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit',\n",
    "        'Asymmetrique Activity Score', 'Asymmetrique Profile Score'\n",
    "    ]\n",
    "\n",
    "    ordinal_cols = [\n",
    "        'Asymmetrique Activity Index', 'Asymmetrique Profile Index', 'Lead Quality'\n",
    "    ]\n",
    "    ordinal_map = [\n",
    "        [1, 2, 3],                        # Activity Index (Low=1, Medium=2, High=3)\n",
    "        [1, 2, 3],                        # Profile Index\n",
    "        ['Low', 'Medium', 'High', 'Missing']  # Lead Quality\n",
    "    ]\n",
    "\n",
    "    categorical_cols = [\n",
    "        'Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Last Activity',\n",
    "        'Country', 'Specialization', 'How did you hear about X Education',\n",
    "        'What is your current occupation', 'What matters most to you in choosing a course',\n",
    "        'Tags', 'Lead Profile', 'City',\n",
    "        'A free copy of Mastering The Interview', 'Last Notable Activity'\n",
    "    ]\n",
    "\n",
    "    # ----------------------\n",
    "    # 2. Train-Test Split\n",
    "    # ----------------------\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # ----------------------\n",
    "    # 3. Winsorize outliers\n",
    "    # ----------------------\n",
    "    X_train = winsorize_columns(X_train.copy(), numeric_cols)\n",
    "    X_test = winsorize_columns(X_test.copy(), numeric_cols)\n",
    "\n",
    "    # ----------------------\n",
    "    # 4. Build Transformers\n",
    "    # ----------------------\n",
    "\n",
    "    # Numeric Pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('yeojohnson', PowerTransformer(method='yeo-johnson')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    # Ordinal Pipeline\n",
    "    ordinal_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(categories=ordinal_map))\n",
    "    ])\n",
    "\n",
    "    # Categorical Pipeline\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "\n",
    "    # Combine all\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('ord', ordinal_transformer, ordinal_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    # ----------------------\n",
    "    # 5. Fit + Transform\n",
    "    # ----------------------\n",
    "    X_train_trans = preprocessor.fit_transform(X_train)\n",
    "    X_test_trans = preprocessor.transform(X_test)\n",
    "\n",
    "    # ----------------------\n",
    "    # 6. Save Preprocessor\n",
    "    # ----------------------\n",
    "    joblib.dump(preprocessor, os.path.join(save_dir, \"preprocessor.pkl\"))\n",
    "\n",
    "    # ----------------------\n",
    "    # 7. Get Feature Names\n",
    "    # ----------------------\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    feature_names = numeric_cols + ordinal_cols + list(cat_feature_names)\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_trans, columns=feature_names, index=X_train.index)\n",
    "    X_test_df = pd.DataFrame(X_test_trans, columns=feature_names, index=X_test.index)\n",
    "\n",
    "    print(\"✅ Preprocessing complete. Artifacts saved to:\", save_dir)\n",
    "    print(\"🧾 X_train shape:\", X_train_df.shape)\n",
    "    print(\"🧾 X_test shape :\", X_test_df.shape)\n",
    "    print(\"🎯 y_train dist:\\n\", y_train.value_counts(normalize=True))\n",
    "\n",
    "    return X_train_df, X_test_df, y_train, y_test\n",
    "\n",
    "\n",
    "# To test directly\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df_featured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ec9a0",
   "metadata": {},
   "source": [
    "Selecting best model based on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f595f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic_regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:23:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:23:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run logistic_regression_run at: http://localhost:5000/#/experiments/199496956156630131/runs/feb0b59162c64f2aba1c95870830c09b.\n",
      "2025/07/17 14:23:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/199496956156630131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:23:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:23:41 INFO mlflow.tracking._tracking_service.client: 🏃 View run random_forest_run at: http://localhost:5000/#/experiments/199496956156630131/runs/5775b36cf13d4e1fb22821a5f9749b24.\n",
      "2025/07/17 14:23:41 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/199496956156630131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "2025/07/17 14:23:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:23:49 INFO mlflow.tracking._tracking_service.client: 🏃 View run xgboost_run at: http://localhost:5000/#/experiments/199496956156630131/runs/844f22aa58df445da7f171d61c7ccbcb.\n",
      "2025/07/17 14:23:49 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/199496956156630131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lightgbm...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2546, number of negative: 3821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 6367, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.399874 -> initscore=-0.405989\n",
      "[LightGBM] [Info] Start training from score -0.405989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:23:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:23:58 INFO mlflow.tracking._tracking_service.client: 🏃 View run lightgbm_run at: http://localhost:5000/#/experiments/199496956156630131/runs/b9f3bc46114f4b3fa20206b5946f0515.\n",
      "2025/07/17 14:23:58 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/199496956156630131.\n",
      "Registered model 'LeadConversionModel' already exists. Creating a new version of this model...\n",
      "2025/07/17 14:23:58 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LeadConversionModel, version 3\n",
      "Created version '3' of model 'LeadConversionModel'.\n",
      "``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: xgboost\n",
      "Test Accuracy: 0.9271\n",
      "Test F1 Score: 0.9075\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       955\n",
      "           1       0.92      0.89      0.91       637\n",
      "\n",
      "    accuracy                           0.93      1592\n",
      "   macro avg       0.93      0.92      0.92      1592\n",
      "weighted avg       0.93      0.93      0.93      1592\n",
      "\n",
      "\n",
      "Model saved to: pkl_joblib_files\\model.pkl\n",
      "Model version 3 moved to Staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version 3 moved to Production\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_evaluate_and_select_model(X_train, y_train, X_test, y_test, save_dir=\"pkl_joblib_files\", model_name=\"LeadConversionModel\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Set MLflow tracking URI and experiment\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"lead_conversion_experiment\")\n",
    "\n",
    "    models = {\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['lbfgs']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'model': LGBMClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [-1, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_name = None\n",
    "    best_run_id = None\n",
    "\n",
    "    for name, config in models.items():\n",
    "        with mlflow.start_run(run_name=f\"{name}_run\") as run:\n",
    "            print(f\"Training {name}...\")\n",
    "\n",
    "            grid = GridSearchCV(config['model'], config['params'], \n",
    "                                cv=5, scoring='f1', \n",
    "                                n_jobs=-1, verbose=0)\n",
    "            grid.fit(X_train, y_train)\n",
    "\n",
    "            f1_cv = grid.best_score_\n",
    "            mlflow.log_params(grid.best_params_)\n",
    "            mlflow.log_metric(\"f1_cv\", f1_cv)\n",
    "\n",
    "            mlflow.sklearn.log_model(grid.best_estimator_, artifact_path=\"model\")\n",
    "\n",
    "            if f1_cv > best_score:\n",
    "                best_score = f1_cv\n",
    "                best_model = grid.best_estimator_\n",
    "                best_name = name\n",
    "                best_run_id = run.info.run_id\n",
    "\n",
    "    # Final evaluation on test data\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"\\nBest Model: {best_name}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "    # Save model locally\n",
    "    model_path = os.path.join(save_dir, \"model.pkl\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "\n",
    "    # Register model in MLflow\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    result = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    # Transition to Staging\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=result.version,\n",
    "        stage=\"Staging\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"Model version {result.version} moved to Staging\")\n",
    "\n",
    "    # Transition to Production\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=result.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"Model version {result.version} moved to Production\")\n",
    "\n",
    "    return best_model, y_pred_test\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    best_model, y_pred_test = train_evaluate_and_select_model(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd228f92",
   "metadata": {},
   "source": [
    "Selecting best model based on Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b204a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Tuning logistic_regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:24:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:24:45 INFO mlflow.tracking._tracking_service.client: 🏃 View run logistic_regression_run at: http://localhost:5000/#/experiments/410342575703778846/runs/5ea10dd6481942cabe039541e91ce2fe.\n",
      "2025/07/17 14:24:45 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/410342575703778846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Tuning random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:25:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:25:13 INFO mlflow.tracking._tracking_service.client: 🏃 View run random_forest_run at: http://localhost:5000/#/experiments/410342575703778846/runs/33c09cc911e94c5eaf63ca1a82295599.\n",
      "2025/07/17 14:25:13 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/410342575703778846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Tuning xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:25:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "2025/07/17 14:25:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:25:39 INFO mlflow.tracking._tracking_service.client: 🏃 View run xgboost_run at: http://localhost:5000/#/experiments/410342575703778846/runs/aa77914f340b403d981f012ffcd700db.\n",
      "2025/07/17 14:25:39 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/410342575703778846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Tuning lightgbm...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2546, number of negative: 3821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 6367, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.399874 -> initscore=-0.405989\n",
      "[LightGBM] [Info] Start training from score -0.405989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 14:25:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 14:25:59 INFO mlflow.tracking._tracking_service.client: 🏃 View run lightgbm_run at: http://localhost:5000/#/experiments/410342575703778846/runs/30447a9e24a94ebbaf9b4838801c9c22.\n",
      "2025/07/17 14:25:59 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/410342575703778846.\n",
      "Registered model 'LeadConversionClassifier' already exists. Creating a new version of this model...\n",
      "2025/07/17 14:26:00 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LeadConversionClassifier, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: xgboost\n",
      "🔢 Accuracy: 0.8763\n",
      "🎯 Precision (class 1): 0.9564\n",
      "📊 F1 Score: 0.8239\n",
      "\n",
      "📝 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       955\n",
      "           1       0.96      0.72      0.82       637\n",
      "\n",
      "    accuracy                           0.88      1592\n",
      "   macro avg       0.90      0.85      0.86      1592\n",
      "weighted avg       0.89      0.88      0.87      1592\n",
      "\n",
      "📁 Model saved to: pkl_joblib_files\\best_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'LeadConversionClassifier'.\n",
      "``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Model version 2 moved to Staging\n",
      "🏁 Model version 2 moved to Production\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def train_and_register_best_classifier(X_train, y_train, X_test, y_test, save_dir=\"pkl_joblib_files\", model_name=\"LeadConversionClassifier\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"Lead_Conversion_Classification\")\n",
    "\n",
    "    models = {\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['lbfgs']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.05, 0.1]\n",
    "            }\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'model': LGBMClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [-1, 5, 10],\n",
    "                'learning_rate': [0.01, 0.05, 0.1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_precision = 0\n",
    "    best_name = None\n",
    "    best_run_id = None\n",
    "\n",
    "    for name, config in models.items():\n",
    "        with mlflow.start_run(run_name=f\"{name}_run\") as run:\n",
    "            print(f\"🔍 Tuning {name}...\")\n",
    "\n",
    "            grid = GridSearchCV(\n",
    "                config['model'], config['params'],\n",
    "                cv=5, scoring='precision', n_jobs=-1\n",
    "            )\n",
    "            grid.fit(X_train, y_train)\n",
    "\n",
    "            # Predictions on test set\n",
    "            y_pred = grid.best_estimator_.predict(X_test)\n",
    "            class1_precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "            mlflow.log_params(grid.best_params_)\n",
    "            mlflow.log_metric(\"test_precision_class1\", class1_precision)\n",
    "            mlflow.sklearn.log_model(grid.best_estimator_, artifact_path=\"model\")\n",
    "\n",
    "            if class1_precision > best_precision:\n",
    "                best_model = grid.best_estimator_\n",
    "                best_precision = class1_precision\n",
    "                best_name = name\n",
    "                best_run_id = run.info.run_id\n",
    "\n",
    "    # Final evaluation\n",
    "    y_final_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_final_pred)\n",
    "    f1 = f1_score(y_test, y_final_pred)\n",
    "    precision = precision_score(y_test, y_final_pred, pos_label=1)\n",
    "\n",
    "    print(f\"✅ Best Model: {best_name}\")\n",
    "    print(f\"🔢 Accuracy: {acc:.4f}\")\n",
    "    print(f\"🎯 Precision (class 1): {precision:.4f}\")\n",
    "    print(f\"📊 F1 Score: {f1:.4f}\")\n",
    "    print(\"\\n📝 Classification Report:\\n\", classification_report(y_test, y_final_pred))\n",
    "\n",
    "    # Save model locally\n",
    "    model_path = os.path.join(save_dir, \"best_model.pkl\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"📁 Model saved to: {model_path}\")\n",
    "\n",
    "    # Register best model to MLflow Model Registry\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    result = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    # Transition to Staging and then Production\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name, version=result.version, stage=\"Staging\", archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"🚀 Model version {result.version} moved to Staging\")\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name, version=result.version, stage=\"Production\", archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"🏁 Model version {result.version} moved to Production\")\n",
    "\n",
    "    return best_model, y_final_pred\n",
    "\n",
    "\n",
    "# Example usage (you need to define `X_train`, `y_train`, `X_test`, `y_test` beforehand)\n",
    "if __name__ == \"__main__\":\n",
    "    best_model, y_pred_test = train_and_register_best_classifier(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df004476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Using existing experiment 'Lead Conversion Prediction Evidently' (ID=321335971120077205)\n",
      "📄 Logged HTML: evidently_train_vs_test_2025-07-17_16-13-25.html\n",
      "🗄️  Logged JSON: evidently_train_vs_test_2025-07-17_16-13-25.json\n",
      "🔢 drifted_columns_count = 0.0\n",
      "🔢 drifted_columns_share = 0.0\n",
      "🔢 dataset_row_count = 1848.0\n",
      "🔢 dataset_column_count = 36.0\n",
      "🔢 drift_Asymmetrique Activity Score = 0.08154543204692331\n",
      "🔢 drift_Asymmetrique Profile Score = 0.03366386451690208\n",
      "🔢 drift_Lead Number = 0.03991236408159937\n",
      "🔢 drift_Page Views Per Visit = 0.04395116701824102\n",
      "🔢 drift_Total Time Spent on Website = 0.009859456860784861\n",
      "🔢 drift_TotalVisits = 0.036930827545720725\n",
      "🔢 drift_A free copy of Mastering The Interview = 0.008688713708195937\n",
      "🔢 drift_Asymmetrique Activity Index = 0.03248798683593431\n",
      "🔢 drift_Asymmetrique Profile Index = 0.012893295537662212\n",
      "🔢 drift_City = 0.026200523954586385\n",
      "🔢 drift_Country = 0.07000374873909854\n",
      "🔢 drift_Digital Advertisement = 0.0030139568605141265\n",
      "🔢 drift_Do Not Call = 0.007906790823558121\n",
      "🔢 drift_Do Not Email = 0.005877170652690993\n",
      "🔢 drift_Get updates on DM Content = 0.0\n",
      "🔢 drift_How did you hear about X Education = 0.03536272862598392\n",
      "🔢 drift_I agree to pay the amount through cheque = 0.0\n",
      "🔢 drift_Last Activity = 0.04196986456962824\n",
      "🔢 drift_Last Notable Activity = 0.03194476654230158\n",
      "🔢 drift_Lead Origin = 0.027178022805285747\n",
      "🔢 drift_Lead Profile = 0.021793392933361015\n",
      "🔢 drift_Lead Quality = 0.030406422608334923\n",
      "🔢 drift_Lead Source = 0.056182521811663604\n",
      "🔢 drift_Magazine = 0.0\n",
      "🔢 drift_Newspaper = 0.007906790823558121\n",
      "🔢 drift_Newspaper Article = 0.006870717766710859\n",
      "🔢 drift_Receive More Updates About Our Courses = 0.0\n",
      "🔢 drift_Search = 0.005222667264397203\n",
      "🔢 drift_Specialization = 0.0514296691653918\n",
      "🔢 drift_Tags = 0.062287911088687004\n",
      "🔢 drift_Through Recommendations = 0.020923487306248607\n",
      "🔢 drift_Update me on Supply Chain Content = 0.0\n",
      "🔢 drift_What is your current occupation = 0.018192032861780515\n",
      "🔢 drift_What matters most to you in choosing a course = 0.02296513638212923\n",
      "🔢 drift_X Education Forums = 0.007906790823558121\n",
      "🔢 drift_Prospect ID = 0.49964194881576857\n",
      "✅ All requested drift & dataset metrics logged to MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 16:13:28 INFO mlflow.tracking._tracking_service.client: 🏃 View run Preprocessing and Tuning at: http://localhost:5000/#/experiments/321335971120077205/runs/2eb672ca7d1e4f0aa1a14b2c80e7594f.\n",
      "2025/07/17 16:13:28 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/321335971120077205.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "import json\n",
    "from pathlib import Path\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def split_data(df, target_column=\"Converted\"):\n",
    "    \"\"\"\n",
    "    Splits the input DataFrame into train, validation, and test sets (X and y).\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # First split: Train vs Temp (Val+Test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Second split: Val vs Test (from temp)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def log_evidently_report(reference_data, current_data, dataset_name=\"train_vs_test\"):\n",
    "    \n",
    "    #  Align columns: use only the intersection to avoid partial-column errors\n",
    "    common_cols = set(reference_data.columns).intersection(current_data.columns)\n",
    "    if not common_cols:\n",
    "        print(f\"⚠️ No common columns between reference and {dataset_name}; skipping Evidently report.\")\n",
    "        return\n",
    "    ref = reference_data[sorted(common_cols)]\n",
    "    cur = current_data[sorted(common_cols)]\n",
    "\n",
    "    #  Run the Evidently report (drift + summary)\n",
    "    report = Report(metrics=[DataDriftPreset(), DataSummaryPreset()])\n",
    "    result = report.run(reference_data=ref, current_data=cur)\n",
    "\n",
    "    #  Ensure local save directory exists\n",
    "    save_dir = Path.cwd() / \"evidently_reports\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #  Save HTML and JSON\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    html_path = save_dir / f\"evidently_{dataset_name}_{ts}.html\"\n",
    "    json_path = save_dir / f\"evidently_{dataset_name}_{ts}.json\"\n",
    "    result.save_html(str(html_path))\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(result.json())\n",
    "\n",
    "    #  Log artifacts to MLflow\n",
    "    mlflow.log_artifact(str(html_path), artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(str(json_path), artifact_path=\"evidently\")\n",
    "    print(f\"📄 Logged HTML: {html_path.name}\")\n",
    "    print(f\"🗄️  Logged JSON: {json_path.name}\")\n",
    "\n",
    "    #  Load JSON and extract metrics list\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        report_json = json.load(fp)\n",
    "    metrics_list = report_json.get(\"metrics\", [])\n",
    "\n",
    "    #  Overall drifted columns metrics\n",
    "    drift_entry = next((m for m in metrics_list if m.get(\"metric_id\", \"\").startswith(\"DriftedColumnsCount\")), None)\n",
    "    if drift_entry:\n",
    "        count = drift_entry[\"value\"][\"count\"]\n",
    "        share = drift_entry[\"value\"][\"share\"]\n",
    "        mlflow.log_metric(\"drifted_columns_count\", float(count))\n",
    "        mlflow.log_metric(\"drifted_columns_share\", float(share))\n",
    "        print(f\"🔢 drifted_columns_count = {count}\")\n",
    "        print(f\"🔢 drifted_columns_share = {share}\")\n",
    "    else:\n",
    "        print(\"⚠️ No DriftedColumnsCount entry found.\")\n",
    "\n",
    "    #  Row and column counts\n",
    "    rowcount = next((m[\"value\"] for m in metrics_list if m.get(\"metric_id\") == \"RowCount()\"), None)\n",
    "    colcount = next((m[\"value\"] for m in metrics_list if m.get(\"metric_id\") == \"ColumnCount()\"), None)\n",
    "    if rowcount is not None:\n",
    "        mlflow.log_metric(\"dataset_row_count\", float(rowcount))\n",
    "        print(f\"🔢 dataset_row_count = {rowcount}\")\n",
    "    if colcount is not None:\n",
    "        mlflow.log_metric(\"dataset_column_count\", float(colcount))\n",
    "        print(f\"🔢 dataset_column_count = {colcount}\")\n",
    "\n",
    "    #  Per-feature value drift metrics\n",
    "    for m in metrics_list:\n",
    "        mid = m.get(\"metric_id\", \"\")\n",
    "        if mid.startswith(\"ValueDrift(column=\"):\n",
    "            # extract column name\n",
    "            col = mid.split(\"=\")[1].rstrip(\")\")\n",
    "            val = m.get(\"value\")\n",
    "            if isinstance(val, (int, float)):\n",
    "                mlflow.log_metric(f\"drift_{col}\", float(val))\n",
    "                print(f\"🔢 drift_{col} = {val}\")\n",
    "    \n",
    "    print(\"✅ All requested drift & dataset metrics logged to MLflow.\")\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"Lead Conversion Prediction Evidently\"\n",
    "\n",
    "def main():\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # ─── 1️⃣ Ensure the MLflow experiment exists and is active ───\n",
    "    exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if exp is None:\n",
    "        exp_id = client.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"✅ Created new experiment '{EXPERIMENT_NAME}' (ID={exp_id})\")\n",
    "    elif exp.lifecycle_stage == \"deleted\":\n",
    "        client.restore_experiment(exp.experiment_id)\n",
    "        print(f\"🔄 Restored deleted experiment '{EXPERIMENT_NAME}' (ID={exp.experiment_id})\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Using existing experiment '{EXPERIMENT_NAME}' (ID={exp.experiment_id})\")\n",
    "\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    # ─── 2️⃣ Start your MLflow run ───\n",
    "    with mlflow.start_run(run_name=\"Lead Conversion Prediction Drift Detection\"):\n",
    "        # Load and split\n",
    "        df = load_data()\n",
    "        Xtr, Xv, Xt, ytr, yv, yt = split_data(df)\n",
    "\n",
    "        # Keep raw for Evidently\n",
    "        df_train = Xtr.copy()\n",
    "        df_test  = Xt.copy()\n",
    "\n",
    "        df_train = df_train.dropna(axis=1, how='all')\n",
    "        df_test = df_test.dropna(axis=1, how='all')\n",
    "\n",
    "        # Log Evidently reports\n",
    "        log_evidently_report(df_train, df_test, dataset_name=\"train_vs_test\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ins-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
